{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis with Deep Learning using BERT","metadata":{"id":"5793KeXDLQHo"}},{"cell_type":"markdown","source":"## Exploratory Data Analysis and Preprocessing","metadata":{"id":"KkeOSBcOLQH8"}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm.notebook import tqdm","metadata":{"id":"-KPmp_zlLQIA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('https://raw.githubusercontent.com/campusx-team/Text-Datasets/refs/heads/main/Reddit_Data.csv')\ndf.dropna(inplace=True)\ndf.index.name = 'id'","metadata":{"id":"KCmfBCOVLQIP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.rename(columns={'clean_comment':'text'},inplace=True)","metadata":{"id":"WVv3wypp6Pth","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"id":"uMUbnE1n5u_5","outputId":"da4e0968-b148-4c95-947a-00fa47cefce0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.text.iloc[0]","metadata":{"id":"RoTwpkHRLQIZ","outputId":"ff6cda24-d11f-4224-9ba6-431c69c3b359","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.category.value_counts()","metadata":{"id":"NBvKl_Z4LQIq","outputId":"2d64a9ec-d3f2-4448-f237-eec32a78e04c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.category.value_counts()","metadata":{"id":"aXMamUsZLQJJ","outputId":"6b483616-df31-4ec2-8b5f-0b9e072e4a9f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"possible_labels = df.category.unique()","metadata":{"id":"LSokWi-ZLQJT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"possible_labels.sort()","metadata":{"id":"2Fz0Y-_y6p_A","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -1:0, 0:1, 1:2\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index","metadata":{"id":"Q5-UFriwLQJb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['label'] = df.category.replace(label_dict)","metadata":{"id":"gsS9pwvdLQJk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"id":"06caCGYPLQJv","outputId":"39749f3b-639e-4c4f-d830-fd0e94b88d1a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"id":"9047IjGR7Lia","outputId":"5b22103f-1938-4889-fe91-bad817553a65","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training/Validation Split","metadata":{"id":"1YZt9WXcLQJ3"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"id":"orbKGYnILQJ5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(df.index.values, df.label.values, test_size = 0.15, random_state=17, stratify = df.label.values)","metadata":{"id":"m8uTipI5LQKB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['data_type'] = ['not_set']*df.shape[0]","metadata":{"id":"2fPzQfobLQKJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"id":"lkPyFD9iLQKS","outputId":"a087ca48-af27-4fc8-e1a9-a5e786a5a39b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'","metadata":{"id":"ommAiv1YLQKe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.groupby(['category', 'label', 'data_type']).count()","metadata":{"id":"m01mkyf8LQKq","outputId":"1ad822ce-ecee-424a-b647-9f3e0baaca08","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Loading Tokenizer and Encoding our Data","metadata":{"id":"LGo4qqv2LQKy"}},{"cell_type":"code","source":"# !pip install transformers","metadata":{"id":"P36pyp42W7yW","outputId":"6953684a-b2a8-4d71-9242-c89467a594e6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nprint(sys.executable)\n","metadata":{"id":"n_kHT-FBGr7p","outputId":"b867b916-9727-4f5a-af7a-53adb2b78211","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m pip install transformers","metadata":{"id":"ZrH5ZNEFHSsI","outputId":"e25ea190-30be-4467-f6a3-fb2b8e3710c6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset","metadata":{"id":"Q2MLzKwvLQK0","outputId":"34d7dd6d-b690-4db8-91c5-df1d995102a9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"id":"4UTcFY0VLQK7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_data_train = tokenizer.batch_encode_plus(df[df.data_type=='train'].text.values, add_special_tokens=True,return_attention_mask=True,pad_to_max_length=True,max_length=256,return_tensors='pt')","metadata":{"id":"JGB8JeRnLQLD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_data_val= tokenizer.batch_encode_plus(df[df.data_type=='val'].text.values, add_special_tokens=True,return_attention_mask=True,pad_to_max_length=True,max_length=256,return_tensors='pt')","metadata":{"id":"JeQBRM8ZLQLU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(df[df.data_type=='train'].label.values)","metadata":{"id":"BZB05sgOLQLc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids_val = encoded_data_val['input_ids']\nattention_masks_val= encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df[df.data_type=='val'].label.values)","metadata":{"id":"o6Z14b9ILQLm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","metadata":{"id":"Kc-RaMOtLQLt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset_train)","metadata":{"id":"WMXiMqOALQL0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset_val)","metadata":{"id":"ZSN0wGkdLQL-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Setting up BERT Pretrained Model","metadata":{"id":"Y79K5NXlLQMI"}},{"cell_type":"code","source":"","metadata":{"id":"vE9upwG8LQMK"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertForSequenceClassification","metadata":{"id":"opaspziCLQMW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = len(label_dict), output_attentions=False,output_hidden_states=False)","metadata":{"id":"fw2q_F6PLQMf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Creating Data Loaders","metadata":{"id":"_JTXXzjGLQMm"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler","metadata":{"id":"gVjsBzXeLQMo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 16 #32\n\ndataloader_train = DataLoader(dataset_train,sampler=RandomSampler(dataset_train),batch_size=batch_size)","metadata":{"id":"vZuLqoukLQMw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndataloader_val = DataLoader(dataset_val,sampler=RandomSampler(dataset_val),batch_size=32)","metadata":{"id":"9SCL_JVPLQM6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Setting Up Optimizer and Scheduler","metadata":{"id":"I-Y8BcgYLQNA"}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup","metadata":{"id":"Q5nCT33mLQNC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),lr=1e-5, #2e-5 > 5e-5\n                 eps=1e-8)","metadata":{"id":"5Pr_JUumLQNL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs=10\n\nscheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=len(dataloader_train)*epochs)","metadata":{"id":"dW7WNzOdLQNS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Defining our Performance Metrics","metadata":{"id":"QmSqAJQiLQNZ"}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score","metadata":{"id":"uJdsXm-lLQNb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","metadata":{"id":"n4UgLs0_LQN6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy_per_class(preds, labels):\n    labels_dict_inverse = {v: k for k,v in label_dict.items()}\n\n    preds_flat = np.argmax(preds,axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {labels_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')","metadata":{"id":"6_BD0jPrLQOI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating our Training Loop","metadata":{"id":"JXFjptHYLQOP"}},{"cell_type":"code","source":"import random\n\nseed_val = 8\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"id":"CQrIavpHLQOS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m pip install --upgrade pip\n# %pip install torch --index-url https://download.pytorch.org/whl/cu125\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)  # Check PyTorch version\nprint(torch.version.cuda)  # Check the CUDA version PyTorch is using\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nprint(device)","metadata":{"id":"b7I-g-FzLQOh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(dataloader_val):\n\n    model.eval()\n\n    loss_val_total = 0\n    predictions, true_vals = [], []\n\n    for batch in dataloader_val:\n\n        batch = tuple(b.to(device) for b in batch)\n\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n\n    loss_val_avg = loss_val_total/len(dataloader_val)\n\n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n\n    return loss_val_avg, predictions, true_vals\n","metadata":{"id":"6hMpepwPLQOp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nfrom sklearn.metrics import accuracy_score\n\n\n# Define checkpoint directory\ncheckpoint_dir = '/kaggle/working/checkpoints'\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Initialize variables to track the best validation loss\nbest_val_loss = float('inf')\n\nfor epoch in tqdm(range(1, epochs + 1)):\n    \n    model.train()\n    \n    loss_train_total = 0\n    correct_predictions = 0  # To keep track of correct predictions\n    total_predictions = 0     # To keep track of total predictions\n    \n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    \n    for batch in progress_bar:\n        \n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {\n            'input_ids': batch[0],\n            'attention_mask': batch[1],\n            'labels': batch[2]\n        }\n        \n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        \n        # Get predictions\n        preds = torch.argmax(outputs[1], dim=1)  # Assuming the logits are in outputs[1]\n        \n        # Update correct and total counts\n        correct_predictions += (preds == batch[2]).sum().item()\n        total_predictions += batch[2].size(0)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total / len(dataloader_train)\n    training_accuracy = correct_predictions / total_predictions  # Calculate training accuracy\n    \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    tqdm.write(f'Training Accuracy: {training_accuracy:.3f}')  # Print training accuracy\n    \n    # Evaluate on the validation set\n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n    val_f1 = f1_score_func(predictions, true_vals)\n    \n    # Calculate validation accuracy\n    predictions = np.argmax(predictions, axis=1).flatten()\n    val_accuracy = accuracy_score(true_vals.flatten(), predictions)\n    \n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (weighted): {val_f1}')\n    tqdm.write(f'Validation Accuracy: {val_accuracy:.3f}')\n    \n    # Save checkpoint if the validation loss improved\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'loss': val_loss\n        }, checkpoint_path)\n        tqdm.write(f'Checkpoint saved at {checkpoint_path}')\n\n# Save the final model after training\nfinal_model_path = './final_model.pth'\ntorch.save(model.state_dict(), final_model_path)\ntqdm.write(f'Final model saved at {final_model_path}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Loading and Evaluating our Model","metadata":{"id":"nmvsA23qLQO7"}},{"cell_type":"code","source":"_, predictions, true_vals = evaluate(dataloader_val)","metadata":{"id":"VppPVTstLQPI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy_per_class(predictions, true_vals )","metadata":{"id":"egPlZoUhLQPR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Calculate validation accuracy\npredictions = np.argmax(predictions, axis=1).flatten()\ntrue_vals = true_vals.flatten()\n\n# Generate classification report\nreport = classification_report(true_vals, predictions, target_names=['Class 0', 'Class 1', 'Class 2'])  # Change class names as per your dataset\nprint(report)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}